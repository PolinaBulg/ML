# Понижение размерности. Отбор признаков. Извлечение признаков
import pandas as pd
data = pd.read_csv('bank_churners_preprocessed.csv')
data.drop(['Unnamed: 0'], axis=1, inplace=True)
data.head()
data = data.sort_values(by='Attrition_Flag', ascending=False)
data = data[:(data[data['Attrition_Flag'] == 1].shape[0]*2)]
data.shape
data[data['Attrition_Flag'] == 0].shape, data[data['Attrition_Flag'] == 1].shape
data = data.sample(frac=1)
X = data.drop(['Attrition_Flag'], axis=1)
y = data['Attrition_Flag']
# Тестировать модель будем с помощью ансамбля
from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
def test(X, y):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=False)
  bag = BaggingClassifier().fit(X_train, y_train)
  print(classification_report(y_test, bag.predict(X_test)))
import numpy as np

np.random.seed(42)
test(X, y)
 
# 1. Отбор признаков

1.1 Отбираем признаки с высокой дисперсией
X.describe()
from sklearn.feature_selection import VarianceThreshold
# параметр - порог значения дисперсии
# будут отобраны только те признаки, у которых дисперсия выше
vt = VarianceThreshold(2)
X_vt = vt.fit_transform(X)
X_vt.shape
X_vt = pd.DataFrame(X_vt, columns=vt.get_feature_names_out())
X_vt.head()
test(X_vt, y)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
X_scaled.head()
vt = VarianceThreshold(1)
X_vt = vt.fit_transform(X_scaled)
print(X_vt.shape)
test(X_vt, y)

# 1.2 Одномерный отбор признаков
from sklearn.feature_selection import SelectKBest

skb = SelectKBest(k=6)
X_skb = skb.fit_transform(X, y)
test(X_skb, y)
X_skb = pd.DataFrame(X_skb, columns=skb.get_feature_names_out())
X_skb

# 1.3 Рекурсивный отбор признаков

from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier().fit(X, y)

rfe = RFE(estimator=tree, n_features_to_select=4, step=1).fit(X, y)
X_rfe = pd.DataFrame(rfe.transform(X), columns=rfe.get_feature_names_out())
X_rfe
test(X_rfe, y)

# 1.4 Отбор признаков по их значимости

import matplotlib.pyplot as plt

plt.barh(width=tree.feature_importances_, y=X.columns);

# 2. Выделение признаков

# 2.1 Метод главных компонент

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X, y)
X_pca.shape
plt.scatter(X_pca[:,0], X_pca[:,1], c=y);
sum(pca.explained_variance_ratio_)

test(X_pca, y)

pca_2 = PCA(n_components=3)
X_pca_2 = pca_2.fit_transform(X, y)

test(X_pca_2, y)


fig = plt.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(X_pca_2[:,0], X_pca_2[:,1], X_pca_2[:,2], c=y);

2.2 Нелинейные методы выделения признаков

from sklearn.manifold import TSNE, Isomap

tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X_scaled)
plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y);

test(X_tsne, y)
isomap = Isomap(n_components=2)
X_isomap = isomap.fit_transform(X_scaled)
plt.scatter(X_isomap[:,0], X_isomap[:,1], c=y);

# Попробуем применить к результату tSNE алгоритм KMeans
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

kmeans = KMeans(n_clusters=8).fit(X_tsne)
plt.scatter(X_tsne[:,0], X_tsne[:,1], c=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='red', s=200);

silhouette_score(X_tsne, kmeans.labels_)

